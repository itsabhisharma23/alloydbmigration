name: Deploy DAGs to Dev Cloud Composer
on:
  push:
    branches:
      - "feature/ddltest"
    paths:
      - "src/python/cloud_composer/dags/**"
      - "src/python/cloud_composer/ddls/**" # Added path for DDLs

permissions:
  contents: read
  id-token: write

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: ${{ github.event_name == 'pull_request' && 2 || 0 }}

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: "projects/818532436077/locations/global/workloadIdentityPools/composer-wif-dev/providers/github-uf"
          service_account: composer-test-sa@stock-data-ingess.iam.gserviceaccount.com
          create_credentials_file: true
          export_environment_variables: true
          cleanup_credentials: false

      - name: 'Set up Cloud SDK'
        uses: 'google-github-actions/setup-gcloud@v2'
        with:
          version: '>= 363.0.0'

      - name: Sync DAGs to GCS
        run: |
          DATASET_ENV='dev'
          PROJECT_ENV='d'
          find src/python/cloud_composer/dags/ -name "*.yaml" -print0 | xargs -0 -I {} sed -i "s/<PROJECT-ENV>/$PROJECT_ENV/g; s/<DATASET-ENV>/$DATASET_ENV/g" {}
          find src/python/cloud_composer/dags/ -name "*.sql" -print0 | xargs -0 -I {} sed -i "s/<PROJECT-ENV>/$PROJECT_ENV/g; s/<DATASET-ENV>/$DATASET_ENV/g" {}
          gsutil -m rsync -d -c -r src/python/cloud_composer/dags/ gs://dataproc-jdbc/dags

      - name: Execute DDLs on BigQuery
        run: |
          DATASET_ENV='dev'
          PROJECT_ENV='d'

          # Determine the list of modified or added DDL files
          if [[ "${{ github.event_name }}" == 'pull_request' ]]; then
            ddl_files=$(git diff --name-only -r HEAD^1 HEAD -- 'src/python/cloud_composer/ddls/*.ddl')
          else
            ddl_files=$(git diff --name-only ${{ github.event.before }} ${{ github.event.after }} -- 'src/python/cloud_composer/ddls/*.ddl')
          fi

          if [ -z "$ddl_files" ]; then
            echo "No DDL files found. Skipping DDL execution."
          else
            for file in $ddl_files; do
              echo "Processing DDL file: $file"

              # Environment variable replacement
              if ! sed -i "s/<PROJECT-ENV>/$PROJECT_ENV/g" "$file"; then
                echo "ERROR: Failed to replace <PROJECT-ENV> in $file"
                exit 1
              fi

              if ! sed -i "s/<DATASET-ENV>/$DATASET_ENV/g" "$file"; then
                echo "ERROR: Failed to replace <DATASET-ENV> in $file"
                exit 1
              fi

              ddl_content=$(cat "$file")

              # Prevent CREATE OR REPLACE TABLE
              if [[ "$ddl_content" == *'CREATE OR REPLACE TABLE'* ]]; then
                echo "ERROR: CREATE OR REPLACE TABLE detected in $file. This is not allowed for DDLs deployments. Exiting."
                exit 1
              fi

              echo "Executing DDL: $file"

              # Execute DDL and handle errors
              bq query --use_legacy_sql=false < "$file" 2>&1 > bq_output.txt || {
                exit_code="$?"
                if cat bq_output.txt | grep -q 'Already Exists: Table'; then
                  echo "WARNING: Table already exists. Skipping $file"
                else
                  echo "ERROR: Failed to execute DDL: $file (Exit Code: $exit_code)"
                  echo "BQ Error Output (from bq_output.txt):"
                  cat bq_output.txt
                  exit 1
                fi
              }

              echo "DDL: $file executed successfully"
            done
          fi

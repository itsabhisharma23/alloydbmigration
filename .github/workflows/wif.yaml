name: Deploy DAGs to Dev Cloud Composer
on:
  push:
    branches:
      - "feature/ddltest"
    paths:
      - "src/python/cloud_composer/dags/**"
      - "src/python/cloud_composer/ddls/**" # Added path for DDLs

permissions:
  contents: read
  id-token: write

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: "projects/818532436077/locations/global/workloadIdentityPools/composer-wif-dev/providers/github-uf"
          service_account: composer-test-sa@stock-data-ingess.iam.gserviceaccount.com
          create_credentials_file: true
          export_environment_variables: true
          cleanup_credentials: false

      - name: 'Set up Cloud SDK'
        uses: 'google-github-actions/setup-gcloud@v2'
        with:
          version: '>= 363.0.0'
      - name: Sync DAGs to GCS
        run: |
          DATASET_ENV='dev'
          PROJECT_ENV='d'
          find src/python/cloud_composer/dags/ -name "*.yaml" -print0 | xargs -0 -I {} sed -i "s/<PROJECT-ENV>/$PROJECT_ENV/g; s/<DATASET-ENV>/$DATASET_ENV/g" {}
          find src/python/cloud_composer/dags/ -name "*.sql" -print0 | xargs -0 -I {} sed -i "s/<PROJECT-ENV>/$PROJECT_ENV/g; s/<DATASET-ENV>/$DATASET_ENV/g" {}
          gsutil -m rsync -d -c -r src/python/cloud_composer/dags/ gs://dataproc-jdbc/dags

      - name: Execute DDLs on BigQuery
        run: |
          DATASET_ENV='dev'
          PROJECT_ENV='d'
          for ddl_file in src/python/cloud_composer/ddls/*.sql; do
            echo "Executing DDL: $ddl_file"
            bq query --use_legacy_sql=false "$(sed "s/<PROJECT-ENV>/$PROJECT_ENV/g; s/<DATASET-ENV>/$DATASET_ENV/g" $ddl_file)"
          done
